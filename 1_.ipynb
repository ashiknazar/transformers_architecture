{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a3479c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e9ac1",
   "metadata": {},
   "source": [
    "## ðŸ§  4. Example Walkthrough â€” English â†’ French (Toy Example)\n",
    "- Input sentence: \"I love AI\"\n",
    "- Target sentence: \"J'adore l'IA\"\n",
    "\n",
    "___\n",
    "### Step 1: Tokenization & Embedding\n",
    "- Input tokens: [I, love, AI]\n",
    "- embeddings â†’ X = [[0.1, 0.3], [0.4, 0.5], [0.9, 0.7]]\n",
    "\n",
    "```python\n",
    "Final input (X + position): [[0.15, 0.35], [0.45, 0.55], [0.95, 0.75]]\n",
    "\n",
    "___\n",
    "### Step 3: Self-Attention (inside encoder)\n",
    "- Each token attends to others.\n",
    "- Result (contextualized representations):\n",
    "```\n",
    "```python\n",
    "Enc_output = [[0.2, 0.4], [0.5, 0.7], [0.8, 0.9]]\n",
    "```\n",
    "___\n",
    "### Step 4: Decoder (Masked Self-Attention)\n",
    "- The decoder always starts with a special start token <start>.\n",
    "```raw\n",
    "Decoder input = [\"<start>\"]\n",
    "```\n",
    "- This is converted into an embedding:\n",
    "```\n",
    " <start> â†’ [0.1, 0.2]  (example embedding)\n",
    "```\n",
    "- Since this is the first word, thereâ€™s no previous token in the decoder.\n",
    "- Masked self-attention looks at only the tokens that have been generated so far.\n",
    "- Here, itâ€™s just <start>\n",
    "- So the output of masked self-attention is essentially the embedding itself (because nothing else exists yet to attend to).\n",
    "```raw\n",
    "\n",
    "Masked Self-Attention output for <start> = [0.1, 0.2]\n",
    "```\n",
    "### Encoderâ€“Decoder Attention\n",
    "- Now, the decoder needs to look at the input sentence (\"I love AI\") via encoder output vectors.\n",
    "- Letâ€™s say encoder produced (context vectors):\n",
    "```raw\n",
    "Enc_output = [\n",
    "  [0.42, 0.40],  # \"I\"\n",
    "  [0.50, 0.70],  # \"love\"\n",
    "  [0.80, 0.90]   # \"AI\"\n",
    "]\n",
    "```\n",
    "___\n",
    "- The decoderâ€™s query comes from the `<start>` token output of masked self-attention: [0.1, 0.2].\n",
    "- Keys and Values come from encoder outputs (`Enc_output`).\n",
    "- We compute attention weights (query Â· keys â†’ softmax):\n",
    "```raw\n",
    "Scores = [0.1*0.42 + 0.2*0.40, 0.1*0.50 + 0.2*0.70, 0.1*0.80 + 0.2*0.90]\n",
    "       = [0.042 + 0.08, 0.05 + 0.14, 0.08 + 0.18]\n",
    "       = [0.122, 0.19, 0.26]\n",
    "\n",
    "Softmax([0.122, 0.19, 0.26]) â‰ˆ [0.30, 0.33, 0.37]\n",
    "\n",
    "```\n",
    "- Interpretation: The decoder `<start>` token attends 37% to \"AI\", 33% to \"love\", 30% to \"I\" (it decides which input words are most relevant for the first output).\n",
    "- Weighted sum over encoder values:\n",
    "```raw\n",
    "Output = 0.30*[0.42,0.40] + 0.33*[0.50,0.70] + 0.37*[0.80,0.90]\n",
    "       = [0.126 + 0.165 + 0.296, 0.12 + 0.231 + 0.333]\n",
    "       = [0.587, 0.684]\n",
    "```\n",
    "___\n",
    "### Feed-Forward + Softmax\n",
    "- Pass [0.587, 0.684] through a small feed-forward network.\n",
    "- Apply Softmax over the vocabulary â†’ gives probabilities for each possible first word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a6fe43",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
