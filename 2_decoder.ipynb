{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86858230",
   "metadata": {},
   "source": [
    "### Step-by-Step for Decoder-Only (e.g., GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250987c",
   "metadata": {},
   "source": [
    "- Let’s say we want to generate text after this prompt:\n",
    "```\n",
    "Prompt: \"I love\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d77c0e",
   "metadata": {},
   "source": [
    "- We generate one token at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e8baa",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87a4f9",
   "metadata": {},
   "source": [
    "### Tokenization + Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea60cf9",
   "metadata": {},
   "source": [
    "```\n",
    "Tokens: [\"I\", \"love\"] → embeddings: [x_I, x_love]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb651c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233bb669",
   "metadata": {},
   "source": [
    "### Add Positional Encoding\n",
    "```\n",
    "x_I + pos_1, x_love + pos_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388164e6",
   "metadata": {},
   "source": [
    "___\n",
    "### Masked Self-Attention\n",
    "- Each token can attend only to tokens before it (including itself).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a121056",
   "metadata": {},
   "source": [
    "| Query token | Attends to  |\n",
    "| ----------- | ----------- |\n",
    "| \"I\"         | \"I\"         |\n",
    "| \"love\"      | \"I\", \"love\" |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859b38a",
   "metadata": {},
   "source": [
    "- This produces context-aware embeddings for each token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d15361",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e089a71",
   "metadata": {},
   "source": [
    "### Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe52bbc",
   "metadata": {},
   "source": [
    "- Each token embedding passes through a small feed-forward network with residual connections.\n",
    "\n",
    "- The output is a vector representing the token in context.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e687521",
   "metadata": {},
   "source": [
    "### Linear + Softmax\n",
    "- Each output embedding is projected to vocabulary size with a linear layer.\n",
    "\n",
    "- Softmax converts it to probabilities for the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5d8e9",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a473ab2",
   "metadata": {},
   "source": [
    "### Step 1 — Tokenization & Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795060ee",
   "metadata": {},
   "source": [
    "```raw\n",
    "Tokens: [\"I\", \"love\"]\n",
    "Embeddings: \n",
    "x_I = [0.5, 0.1]\n",
    "x_love = [0.4, 0.6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791a637",
   "metadata": {},
   "source": [
    "- Add positional encoding (toy example):\n",
    "```raw\n",
    "x_I + pos_1 = [0.55, 0.1]\n",
    "x_love + pos_2 = [0.4, 0.65]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b2ef8",
   "metadata": {},
   "source": [
    "### Step 2 — Masked Self-Attention\n",
    "\n",
    "- Attention for \"I\": attends to itself only → output = [0.55, 0.1]\n",
    "\n",
    "- Attention for \"love\": attends to \"I\" and \"love\" → output (contextualized) = [0.48, 0.38]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41de223",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc16b1e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
